<div data-github-url="https://github.com/kamranahmedse/developer-roadmap/tree/master/src/data/roadmaps/prompt-engineering/content/104-llm-pitfalls/104-prompt-hacking.md"></div> <h1 id="prompt-hacking">Prompt Hacking</h1>
<p>Prompt hacking is a term used to describe a situation where a model, specifically a language model, is tricked or manipulated into generating outputs that violate safety guidelines or are off-topic. This could include content that’s harmful, offensive, or not relevant to the prompt.</p>
<p>There are a few common techniques employed by users to attempt “prompt hacking,” such as:</p>
<ol>
<li><strong>Manipulating keywords</strong>: Users may introduce specific keywords or phrases that are linked to controversial, inappropriate, or harmful content in order to trick the model into generating unsafe outputs.</li>
<li><strong>Playing with grammar</strong>: Users could purposely use poor grammar, spelling, or punctuation to confuse the model and elicit responses that might not be detected by safety mitigations.</li>
<li><strong>Asking leading questions</strong>: Users can try to manipulate the model by asking highly biased or loaded questions, hoping to get a similar response from the model.</li>
</ol>
<p>To counteract prompt hacking, it’s essential for developers and researchers to build in safety mechanisms such as content filters and carefully designed prompt templates to prevent the model from generating harmful or unwanted outputs. Constant monitoring, analysis, and improvement to the safety mitigations in place can help ensure the model’s output aligns with the desired guidelines and behaves responsibly.</p>
<p>Read more about prompt hacking here <a href="https://learnprompting.org/docs/category/-prompt-hacking" rel="noopener noreferrer nofollow" target="_blank">Prompt Hacking</a>.</p>