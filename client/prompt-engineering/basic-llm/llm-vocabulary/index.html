<div data-github-url="https://github.com/kamranahmedse/developer-roadmap/tree/master/src/data/roadmaps/prompt-engineering/content/100-basic-llm/103-llm-vocabulary.md"></div> <h1 id="vocabulary">Vocabulary</h1>
<p>When working with LLMs, you will come across a lot of new terms. This section will help you understand the meaning of these terms and how they are used in the context of LLMs.</p>
<ul>
<li>
<p><strong>Machine Learning (ML)</strong> — ML is a field of study that focuses on algorithms that can learn from data. ML is a subfield of AI.</p>
</li>
<li>
<p><strong>”Model” vs. “AI” vs. “LLM”</strong> — These terms are used somewhat interchangeably throughout this course, but they do not always mean the same thing. LLMs are a type of AI, as noted above, but not all AIs are LLMs. When we mentioned models in this course, we are referring to AI models. As such, in this course, you can consider the terms “model” and “AI” to be interchangeable.</p>
</li>
<li>
<p><strong>LLM</strong> — Large language model. A large language model is a type of artificial intelligence that can understand and generate human-like text based on the input it receives. These models have been trained on vast amounts of text data and can perform a wide range of language-related tasks, such as answering questions, carrying out conversations, summarizing text, translating languages, and much more.</p>
</li>
<li>
<p><strong>MLM</strong> — Masked language model. A masked language model is a type of language model that is trained to predict the next word in a sequence of words. It is typically trained on a large corpus of text data and can be used for a variety of tasks, such as machine translation, sentiment analysis, summarization, and more.</p>
</li>
<li>
<p><strong>NLP</strong> — Natural language processing. Natural language processing is a branch of artificial intelligence that deals with the interaction between computers and human languages. It is used to analyze, understand, and generate human language.</p>
</li>
<li>
<p><strong>Label</strong> — Labels are just possibilities for the classification of a given text. For example, if you have a text that says “I love you”, then the labels could be “positive”, “negative”, or “neutral”. The model will try to predict which label is most likely to be correct based on the input text.</p>
</li>
<li>
<p><strong>Label Space</strong> — The label space is the set of all possible labels that can be assigned to a given text. For example, if you have a text that says “I love you”, then the label space could be “positive”, “negative”, or “neutral”.</p>
</li>
<li>
<p><strong>Label Distribution</strong> — The label distribution is the probability distribution over the label space. For example, if you have a text that says “I love you”, then the label distribution could be [0.8, 0.1, 0.1]. This means that the model thinks there is an 80% chance that the text is positive, a 10% chance that it is negative, and a 10% chance that it is neutral.</p>
</li>
<li>
<p><strong>Sentiment Analysis</strong> — Sentiment analysis is the process of determining the emotional tone behind a series of words, used to gain an understanding of the attitudes, opinions and emotions expressed within an online mention. Sentiment analysis is also known as opinion mining, deriving the opinion or attitude of a speaker.</p>
</li>
<li>
<p><strong>Verbalizer</strong> — In the classification setting, verbalizers are mappings from labels to words in a language model’s vocabulary. For example, consider performing sentiment classification with the following prompt:</p>
<pre class="astro-code dracula" style="background-color:#282A36;color:#F8F8F2; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Tweet: "I love hotpockets"</span></span>
<span class="line"><span>What is the sentiment of this tweet? Say 'pos' or 'neg'.</span></span>
<span class="line"><span></span></span></code></pre>
<p>Here, the verbalizer is the mapping from the conceptual labels of <code>positive</code> and <code>negative</code> to the tokens <code>pos</code> and <code>neg</code>.</p>
</li>
<li>
<p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong> — RLHF is a technique for training a model to perform a task by providing it with human feedback. The model is trained to maximize the amount of positive feedback it receives from humans, while minimizing the amount of negative feedback it receives.</p>
</li>
</ul>
<p>References and further learning:</p>
<ul>
<li><a href="https://learnprompting.org/docs/vocabulary" rel="noopener noreferrer nofollow" target="_blank">@article@LLM Vocabulary</a></li>
<li><a href="https://app.daily.dev/tags/llm?ref=roadmapsh" rel="noopener noreferrer nofollow" target="_blank">@feed@Explore top posts about LLM</a></li>
</ul>